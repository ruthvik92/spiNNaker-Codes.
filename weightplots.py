
## This code plots the final weights and labels the the neuron numberr
## whose weights are greater than a particular value for eg:0.95

c = [0.14453125, 0.8203125, 0.33203125, 0.0, 0.3671875, 0.2421875, 0.109375, 0.41796875, 0.21484375, 0.18359375, 0.1640625, 0.140625, 0.21484375, 0.0, 0.04296875, 0.7578125, 0.3515625, 0.0, 0.34765625, 1.0, 0.0, 0.09765625, 0.1171875, 0.171875, 0.16015625, 0.16796875, 0.1015625, 0.140625, 0.3671875, 0.0625, 0.375, 0.109375, 0.04296875, 0.0, 0.03515625, 0.2890625, 0.1640625, 0.2109375, 0.140625, 0.20703125, 0.26171875, 0.22265625, 0.0625, 0.0, 0.171875, 0.73828125, 0.140625, 0.28515625, 0.0, 0.21875, 0.1015625, 0.1484375, 0.00390625, 0.33203125, 0.0078125, 0.12109375, 0.0, 0.21484375, 0.171875, 0.00390625, 0.21484375, 0.265625, 0.0, 0.4609375, 0.0, 0.171875, 0.01953125, 0.578125, 0.15234375, 0.24609375, 0.00390625, 0.19140625, 0.54296875, 0.2265625, 0.3203125, 0.1171875, 0.42578125, 0.30859375, 0.0, 0.51171875, 0.3203125, 0.24609375, 0.4921875, 0.3515625, 0.625, 0.2890625, 0.0, 0.44140625, 0.31640625, 0.08984375, 0.35546875, 0.0, 0.0, 0.0703125, 0.0, 0.17578125, 0.41796875, 0.44921875, 0.7265625, 0.08984375, 0.3515625, 0.2734375, 0.390625, 0.1015625, 0.02734375, 0.48046875, 0.20703125, 0.2421875, 0.73828125, 0.0, 0.15234375, 0.0, 0.0, 0.2421875, 0.19140625, 0.078125, 0.78515625, 0.04296875, 0.13671875, 0.1015625, 0.0, 0.26953125, 0.97265625, 0.0, 0.53125, 0.0, 0.02734375, 0.2109375, 0.08203125, 0.09765625, 0.0, 0.23046875, 0.0625, 0.16796875, 0.0, 0.140625, 0.0, 0.00390625, 0.10546875, 0.26171875, 0.29296875, 0.4375, 0.125, 0.23046875, 0.44140625, 0.34375, 0.11328125, 0.421875, 0.0, 0.39453125, 0.375, 0.32421875, 0.109375, 0.46484375, 0.0, 0.0234375, 0.05078125, 0.0, 0.2734375, 0.0, 0.1171875, 0.05078125, 0.4140625, 0.0, 0.33203125, 0.0, 0.17578125, 1.0, 0.62890625, 0.171875, 0.18359375, 0.046875, 0.09765625, 0.11328125, 0.0390625, 0.05859375, 0.17578125, 0.0, 0.1796875, 0.33203125, 0.3125, 0.0, 0.26953125, 0.0, 0.48828125, 0.03515625, 0.15234375, 0.23046875, 0.07421875, 0.0, 0.3515625, 0.0234375, 0.140625, 0.0, 0.453125, 0.0, 0.0, 0.6953125, 1.0, 0.40625, 0.12109375, 0.48828125, 0.296875, 0.35546875, 0.0703125, 0.55859375, 0.2421875, 0.0, 0.1796875, 0.2109375, 0.125, 0.0, 0.3671875, 0.0, 0.0, 0.62109375, 0.0, 0.44921875, 0.05859375, 0.046875, 0.3203125, 0.1015625, 1.0, 0.0390625, 0.296875, 0.1875, 0.203125, 0.453125, 0.14453125, 0.0, 0.0234375, 0.15234375, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.1484375, 0.046875, 0.21875, 0.296875, 0.02734375, 0.0625, 0.02734375, 0.0, 0.29296875, 0.37109375, 0.0, 0.265625, 0.0078125, 0.33203125, 1.0, 0.0, 0.44921875, 0.0, 0.015625, 0.03515625, 0.04296875, 0.24609375, 0.0234375, 0.078125, 0.234375, 0.20703125, 0.21484375, 0.296875, 0.171875, 0.03125, 0.31640625, 0.55859375, 0.234375, 0.0, 0.2578125, 0.109375, 0.2578125, 0.91015625, 0.078125, 0.53125, 0.11328125, 0.0, 0.1640625, 0.26171875, 0.02734375, 0.0, 0.06640625, 0.03125, 0.65625, 0.0234375, 0.04296875, 0.0, 0.07421875, 0.13671875, 0.19140625, 0.53515625, 0.01953125, 0.0, 0.1640625, 0.0078125, 0.14453125, 0.0, 0.50390625, 0.0703125, 0.16796875, 0.0, 0.5859375, 0.75390625, 0.0234375, 0.0, 0.08203125, 0.296875, 0.0, 0.06640625, 0.0, 0.1953125, 0.3359375, 0.01171875, 0.41796875, 0.00390625, 1.0, 0.5625, 0.0, 0.0, 0.3359375, 0.4921875, 0.00390625, 0.13671875, 0.05859375, 0.1640625, 0.03515625, 0.0, 0.59375, 0.296875, 0.48046875, 0.4609375, 0.2421875, 0.09765625, 0.2578125, 0.0390625, 0.0, 0.15625, 0.36328125, 0.0, 0.0, 0.7265625, 0.15234375, 0.08984375, 0.40234375, 0.04296875, 0.0, 0.0, 0.296875, 0.08203125, 0.08984375, 0.265625, 0.2421875, 0.1171875, 0.04296875, 0.04296875, 0.1328125, 0.234375, 0.38671875, 0.078125, 1.0, 0.265625, 0.1875, 0.1796875, 0.375, 0.0, 0.0234375, 0.0, 0.0625, 0.1015625, 0.015625, 0.1875, 0.0, 0.0, 0.6796875, 0.5234375, 0.0, 0.0, 0.078125, 0.13671875, 0.9453125, 0.0, 0.03515625, 0.0, 0.32421875, 0.32421875, 0.0, 0.0, 0.47265625, 0.34765625, 0.37109375, 0.0, 0.296875, 0.33203125, 0.33203125, 0.60546875, 0.26171875, 0.1484375, 0.41015625, 0.34765625, 0.0, 0.17578125, 0.23828125, 0.0546875, 0.0, 0.0, 0.0, 0.890625, 0.0390625, 0.0, 0.1953125, 0.58203125, 0.0, 0.0703125, 0.38671875, 0.03515625, 0.30859375, 0.0078125, 0.27734375, 0.0, 0.64453125, 0.76171875, 0.1484375, 0.0, 0.19921875, 0.203125, 0.26171875, 0.11328125, 0.03125, 0.42578125, 0.26953125, 0.67578125, 0.0, 0.3671875, 0.32421875, 0.0, 0.0, 0.078125, 0.0, 0.1875, 0.49609375, 0.4609375, 0.0, 0.0, 0.0, 0.26953125, 0.19921875, 0.25, 0.3203125, 0.0390625, 0.546875, 0.0, 0.0390625, 0.96484375, 0.0, 0.09765625, 0.0, 0.0, 0.05078125, 0.0, 0.33203125, 0.2734375, 0.1796875, 0.0, 0.0078125, 0.0, 0.1015625, 0.42578125, 0.23046875, 0.0, 0.0078125, 0.0, 0.0703125, 0.0546875, 0.72265625, 0.33203125, 0.0, 0.33203125, 0.0390625, 0.96484375, 0.0859375, 0.02734375, 0.0, 0.0, 0.44921875, 0.1796875, 0.1171875, 0.0, 0.0, 0.0, 0.984375, 0.87890625, 0.00390625, 0.22265625, 0.36328125, 0.05859375, 0.14453125, 0.296875, 0.0, 0.85546875, 0.15234375, 0.34375, 0.015625, 0.3203125, 0.1953125, 0.16796875, 0.35546875, 0.0, 0.4765625, 0.0, 0.26953125, 1.0, 0.0, 0.69140625, 0.1796875, 0.0, 0.32421875, 0.046875, 0.38671875, 0.0, 0.3984375, 0.25390625, 0.0, 0.0, 0.0, 0.29296875, 0.4453125, 0.23828125, 0.97265625, 0.14453125, 0.125, 0.14453125, 0.0, 0.0, 0.515625, 0.23828125, 0.359375, 0.0, 0.0, 0.01171875, 0.2109375, 0.2890625, 0.06640625, 0.3515625, 0.3671875, 0.0, 0.078125, 0.359375, 0.0, 0.1640625, 0.23828125, 0.0, 0.0, 0.11328125, 0.3515625, 0.3359375, 0.16015625, 0.50390625, 0.140625, 0.0, 0.0, 0.23828125, 0.0625, 0.1328125, 0.25, 0.0, 0.0, 0.0, 0.0, 0.07421875, 0.0, 0.2109375, 0.25390625, 0.0, 0.0, 0.0, 0.109375, 0.26171875, 0.04296875, 0.8046875, 0.14453125, 0.515625, 0.046875, 0.0, 0.2578125, 0.09375, 0.21875, 0.17578125, 0.0, 0.3359375, 0.0, 0.0078125, 0.14453125, 0.0, 0.16796875, 0.16015625, 0.50390625, 0.12890625, 0.125, 0.109375, 0.8125, 0.25390625, 0.08984375, 0.0, 0.56640625, 0.4765625, 0.16796875, 0.265625, 0.10546875, 0.140625, 0.17578125, 0.4921875, 0.0, 0.0, 0.171875, 0.4375, 0.14453125, 0.39453125, 0.26953125, 0.41796875, 0.78125, 0.12890625, 0.0, 0.0, 0.02734375, 0.0, 0.0703125, 0.2265625, 0.19140625, 0.0, 0.37890625, 0.140625, 0.50390625, 0.62890625, 0.0078125, 0.0, 0.07421875, 0.37109375, 0.29296875, 0.4765625, 0.046875, 0.12890625, 0.046875, 0.12890625, 0.26171875, 0.01171875, 0.20703125, 0.01171875, 0.33984375, 0.44140625, 0.30859375, 0.3125, 0.015625, 0.26953125, 0.078125, 0.31640625, 0.18359375, 0.26953125, 0.1875, 0.0, 0.3984375, 0.0, 0.04296875, 0.25390625, 0.2734375, 0.01953125, 0.078125, 0.015625, 0.0, 0.1875, 0.0, 0.5703125, 0.078125, 0.0, 0.02734375, 1.0, 0.15625, 0.00390625, 0.33984375, 0.125, 0.24609375, 0.44921875, 1.0, 0.0078125, 0.0, 0.24609375, 0.20703125, 0.01171875, 0.12890625, 0.44140625, 0.0234375, 0.01953125, 0.3515625, 0.09375, 0.4296875, 0.08203125, 0.26171875, 0.01953125, 0.296875]
d = list(range(700))
import matplotlib.pyplot as plt
imp = []
### Change the weight value here
for i in range(len(c)):
    if c[i] >= 0.95:
        imp.append(i)

imp_weights = []
for j in range(len(imp)):
    imp_weights.append(c[imp[j]])

fig, ax = plt.subplots()
ax.scatter(c,d, marker = "+")
ax.scatter(imp_weights,imp, marker="+")

for i, txt in enumerate(imp):
    ax.annotate(txt, (imp_weights[i]+0.01, imp[i]+0.01))
    ax.annotate('Inrease in weights of neurons involved in the pattern', xy=(0.975,400), xytext=(0.6,600),
                arrowprops=dict(facecolor='black', shrink=0.05))

plt.xlabel('Weights', fontsize=18)
plt.ylabel('Neuron ID No.', fontsize=18)
plt.show()
